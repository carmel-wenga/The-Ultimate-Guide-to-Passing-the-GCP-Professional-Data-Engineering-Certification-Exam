# Data Governance
## General Concepts
* Data Governance: Understand Data Regulation accross Europe and U.S such GDPR, HIPAA, PCI-DSS, etc.

## Cloud Dataplex
* Cloud Dataplex: Create virtual lakes and manage zones for different data stages.
* Cloud Dataplex: Enable data mesh by allowing departments to own and share their data zones.
* Cloud Dataplex: Enable decentralized governance allowing domains to manage their own data assets.
* Cloud Dataplex: Govern data access using Dataplex Cloud IAM roles like role/dataplex.dataReader or role.dataplex.dataOwner.
* Cloud Dataplex: Implement a domain-oriented data mesh by organizing lakes by domain.
* Cloud Dataplex: Implement cross-lake sharing for curated data zones.
* Cloud Dataplex: Implement data mesh architecture for unified data view and governance.
* Cloud Dataplex: Manage and discover data across various storage services in a decentralized ecosystem.
* Cloud Dataplex: Manage data access using resource-level permissions for different user groups.
* Cloud Dataplex: Map data lakes to enable data governance and management across departments.
* Cloud Dataplex: Track data lineage to understand data flow and transformations.
* Cloud Dataplex: Understand how to organize Dataplex lakes accross teams and zones.
* Cloud Dataplex: Understand the difference between raw and cureted zones and what resources to attach to each of them.
* Cloud Dataplex: Use exclude patterns to control file discovery and cataloging within Dataplex lakes.
* Cloud Dataplex: Validate data quality using centralized management and governance tools.
* Cloud Dataplex: Configure discovery settings to automatically detect files in raw zones.
* Cloud Dataplex: Organize data using raw and curated zones for different processing stages.
* Cloud Dataplex: Understand the difference between Raw and Curated Zones.

## Cloud DLP (Data Loss Prevention)
* Cloud DLP: Identify and mask sensitive data to comply with privacy requirements using Cloud DLP API
* Cloud DLP: Apply data masking techniques to protect sensitive information before data transmission.
* Cloud DLP: Apply format-preserving encryption for sensitive data protection.
* Cloud DLP: Create an inspection job to identify sensitive infoTypes like STREET_ADDRESS in datasets.
* Cloud DLP: Detect and redact PII from data streams to ensure privacy.
* Cloud DLP: Ensure referential integrity while transforming sensitive data for security.
* Cloud DLP: Identify and redact sensitive data using predefined infoTypes for compliance.
* Cloud DLP: Implement format-preserving encryption to de-identify PII while maintaining data utility.
* Cloud DLP: Implement Google-recommended best practices for data redaction and compliance.
* Cloud DLP: Mask sensitive data in Dataflow pipelines before loading into BigQuery.
* Cloud DLP: Prioritize scanning order based on risk to focus on the most sensitive or at-risk data first.
* Cloud DLP: Run risk analysis to assess re-identification likelihood of sensitive data.
* Cloud DLP: Scan and identify PII to protect sensitive data.
* Cloud DLP: Understand the differences between all the de-identification transformation types of Cloud DLP and their corresponding use cases.
* Cloud DLP: Understand the different data deidentification techniques in Cloud DLP.
* Cloud DLP: Understand what is discovery scan configuration and how to use it.
* Cloud DLP: Understand what is inspection jobs and templates and how to use it.
* Cloud DLP: Use pseudonymization to replace PII data with cryptographic format-preserving tokens.
* Cloud DLP: Utilize privacy controls to protect sensitive information in text files.

## Cloud Data Catalog
* Data Catalog: Automatically catalog datasets and resources across services to enhance data discoverability.
* Data Catalog: Create and manage tags for tagging metadata to improve data asset searchability.
* Data Catalog: Create tag templates to manage metadata and classify datasets.
* Data Catalog: Define policy tags for column-level security enforcement in BigLake tables.
* Data Catalog: Enhance understanding of data assets by associating metadata tags with owner and data description.
* Data Catalog: Manage roles to enforce access control based on data classification.
* Data Catalog: Organize and manage metadata to facilitate data discovery and understanding.
* Data Catalog: Use custom connectors to tag or catalog PostgreSQL data for improved searchability and accessibility.
* Data Catalog: Use data Catalog to automatically tag data across gcp services.
* Data Catalog: Use Data Catalog to enable searching datasets with specific metadata attributes.

